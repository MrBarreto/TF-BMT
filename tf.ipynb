{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando os dados Originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = pd.read_excel('tweets1.xlsx')\n",
    "tweets2 = pd.read_excel('tweets2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_combined = pd.concat([tweets1, tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english = tweets_combined[tweets_combined['Tweet Language'] == 'English'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english['ID'] = range(1, len(tweets_english) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered = tweets_english[['ID', 'Tweet Content']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\\n\\nGet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Itâ€™ll be a tough night for Europe today.\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>\"In defeat or in victory, always say Alhamduli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>\"FAFC Genesis Edition ( This collection have m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Get ready for zabardast action on #25th Jan ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Tweet Content\n",
       "1   1  \"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\\n\\nGet...\n",
       "3   2  \"Itâ€™ll be a tough night for Europe today.\\n\\n#...\n",
       "4   3  \"In defeat or in victory, always say Alhamduli...\n",
       "5   4  \"FAFC Genesis Edition ( This collection have m...\n",
       "6   5  \"Get ready for zabardast action on #25th Jan ...."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered.to_csv('tweets_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando dados na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do CSV usando cudf\n",
    "df = cudf.read_csv('tweets_filtered.csv')\n",
    "\n",
    "# Converter a coluna de tweets para uma lista no cudf (mantendo na GPU)\n",
    "tweets = df['Tweet Content'].head(5000).to_arrow().to_pylist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22f7c79663f41af810e44dd17739edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aaae562e274788a5866cb2c9502a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a551f5d5c9418192351812cc148bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda146185304dac8a72790a6685bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58a55350364e3f9ff9c7efbe382560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95461ec93974f8699a9fd47da463a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a7d8231d2148e8a1afa1e1073c94d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python --prefer-binary --no-cache-dir --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=4,  # NÃºmero de nÃºcleos da CPU\n",
    "    n_batch=512,  # Deve estar entre 1 e n_ctx, considere a quantidade de VRAM na sua GPU\n",
    "    n_gpu_layers=32  # Ajuste com base no modelo e na VRAM da GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcpp_llm.model_params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Read the following tweet inside brackets:\\n\"\n",
    "prompt2 = \"Classify the readed tweet as to whether it mentions sports betting, answer just yes or no.: \"\n",
    "str_for_promp = ''\n",
    "str_for_promp += prompt1 + '['+tweets[0]+']' + '\\n' + prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {str_for_promp}\n",
    "\n",
    "ASSISTANT:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6864.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.82 ms /     9 tokens (   11.42 ms per token,    87.53 tokens per second)\n",
      "llama_print_timings:        eval time =      79.73 ms /     2 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
      "llama_print_timings:       total time =     193.95 ms\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Read the following tweet inside brackets:\n",
      "[\"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\n",
      "\n",
      "Get into the #FIFAWorldCup Final mode with none other than @iamsrk &amp; @WayneRooney on Dec 18, LIVE on #JioCinema &amp; #Sports18 ğŸ“ºğŸ“²\n",
      "\n",
      "#Qatar2022 #ARGFRA #WorldsGreatestShow #FIFAWConJioCinema #FIFAWConSports18 #Pathaan\"]\n",
      "Classify the readed tweet as to whether it mentions sports betting, answer just yes or no.: \n",
      "\n",
      "ASSISTANT:\n",
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.03 ms /     7 runs   (    0.15 ms per token,  6776.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     612.83 ms /   259 tokens (    2.37 ms per token,   422.63 tokens per second)\n",
      "llama_print_timings:        eval time =     225.12 ms /     6 runs   (   37.52 ms per token,    26.65 tokens per second)\n",
      "llama_print_timings:       total time =     850.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6818.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     252.68 ms /    95 tokens (    2.66 ms per token,   375.96 tokens per second)\n",
      "llama_print_timings:        eval time =      69.32 ms /     2 runs   (   34.66 ms per token,    28.85 tokens per second)\n",
      "llama_print_timings:       total time =     326.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.25 ms /     2 runs   (    0.13 ms per token,  7874.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     257.25 ms /   101 tokens (    2.55 ms per token,   392.62 tokens per second)\n",
      "llama_print_timings:        eval time =      34.71 ms /     1 runs   (   34.71 ms per token,    28.81 tokens per second)\n",
      "llama_print_timings:       total time =     295.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     386.46 ms /   182 tokens (    2.12 ms per token,   470.94 tokens per second)\n",
      "llama_print_timings:        eval time =      35.88 ms /     1 runs   (   35.88 ms per token,    27.87 tokens per second)\n",
      "llama_print_timings:       total time =     425.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     3 runs   (    0.14 ms per token,  6976.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.73 ms /    71 tokens (    3.14 ms per token,   318.77 tokens per second)\n",
      "llama_print_timings:        eval time =      67.22 ms /     2 runs   (   33.61 ms per token,    29.75 tokens per second)\n",
      "llama_print_timings:       total time =     294.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.14 ms per token,  6920.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     259.52 ms /   102 tokens (    2.54 ms per token,   393.03 tokens per second)\n",
      "llama_print_timings:        eval time =      35.00 ms /     1 runs   (   35.00 ms per token,    28.57 tokens per second)\n",
      "llama_print_timings:       total time =     297.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6734.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     375.80 ms /   174 tokens (    2.16 ms per token,   463.02 tokens per second)\n",
      "llama_print_timings:        eval time =      35.77 ms /     1 runs   (   35.77 ms per token,    27.95 tokens per second)\n",
      "llama_print_timings:       total time =     414.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.37 ms /     2 runs   (    0.19 ms per token,  5390.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.52 ms /    85 tokens (    2.83 ms per token,   353.40 tokens per second)\n",
      "llama_print_timings:        eval time =      38.95 ms /     1 runs   (   38.95 ms per token,    25.67 tokens per second)\n",
      "llama_print_timings:       total time =     284.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.33 ms /     2 runs   (    0.16 ms per token,  6079.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.25 ms /    88 tokens (    2.74 ms per token,   364.77 tokens per second)\n",
      "llama_print_timings:        eval time =      33.66 ms /     1 runs   (   33.66 ms per token,    29.70 tokens per second)\n",
      "llama_print_timings:       total time =     278.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     256.90 ms /   104 tokens (    2.47 ms per token,   404.83 tokens per second)\n",
      "llama_print_timings:        eval time =      34.64 ms /     1 runs   (   34.64 ms per token,    28.87 tokens per second)\n",
      "llama_print_timings:       total time =     294.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6772.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.16 ms /   259 tokens (    2.22 ms per token,   450.31 tokens per second)\n",
      "llama_print_timings:        eval time =      75.16 ms /     2 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
      "llama_print_timings:       total time =     655.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     221.94 ms /    70 tokens (    3.17 ms per token,   315.39 tokens per second)\n",
      "llama_print_timings:        eval time =      67.76 ms /     2 runs   (   33.88 ms per token,    29.51 tokens per second)\n",
      "llama_print_timings:       total time =     294.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6772.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.65 ms /    92 tokens (    2.71 ms per token,   368.52 tokens per second)\n",
      "llama_print_timings:        eval time =      69.26 ms /     2 runs   (   34.63 ms per token,    28.88 tokens per second)\n",
      "llama_print_timings:       total time =     322.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6688.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     339.97 ms /   149 tokens (    2.28 ms per token,   438.28 tokens per second)\n",
      "llama_print_timings:        eval time =      35.73 ms /     1 runs   (   35.73 ms per token,    27.99 tokens per second)\n",
      "llama_print_timings:       total time =     378.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6779.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     230.51 ms /    76 tokens (    3.03 ms per token,   329.71 tokens per second)\n",
      "llama_print_timings:        eval time =      33.98 ms /     1 runs   (   33.98 ms per token,    29.43 tokens per second)\n",
      "llama_print_timings:       total time =     267.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6637.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.25 ms /   110 tokens (    2.39 ms per token,   417.86 tokens per second)\n",
      "llama_print_timings:        eval time =      68.83 ms /     2 runs   (   34.41 ms per token,    29.06 tokens per second)\n",
      "llama_print_timings:       total time =     336.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6557.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.15 ms /   116 tokens (    2.32 ms per token,   430.99 tokens per second)\n",
      "llama_print_timings:        eval time =      34.60 ms /     1 runs   (   34.60 ms per token,    28.90 tokens per second)\n",
      "llama_print_timings:       total time =     307.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6535.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.23 ms /   122 tokens (    2.24 ms per token,   446.52 tokens per second)\n",
      "llama_print_timings:        eval time =      35.74 ms /     1 runs   (   35.74 ms per token,    27.98 tokens per second)\n",
      "llama_print_timings:       total time =     312.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     363.47 ms /   153 tokens (    2.38 ms per token,   420.95 tokens per second)\n",
      "llama_print_timings:        eval time =      35.67 ms /     1 runs   (   35.67 ms per token,    28.03 tokens per second)\n",
      "llama_print_timings:       total time =     402.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6622.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     382.82 ms /   181 tokens (    2.12 ms per token,   472.80 tokens per second)\n",
      "llama_print_timings:        eval time =      36.30 ms /     1 runs   (   36.30 ms per token,    27.55 tokens per second)\n",
      "llama_print_timings:       total time =     421.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.03 ms /     7 runs   (    0.15 ms per token,  6789.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     582.07 ms /   259 tokens (    2.25 ms per token,   444.97 tokens per second)\n",
      "llama_print_timings:        eval time =     222.40 ms /     6 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =     815.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6578.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     346.78 ms /   153 tokens (    2.27 ms per token,   441.21 tokens per second)\n",
      "llama_print_timings:        eval time =      35.90 ms /     1 runs   (   35.90 ms per token,    27.86 tokens per second)\n",
      "llama_print_timings:       total time =     385.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.40 ms /     3 runs   (    0.13 ms per token,  7500.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.72 ms /    74 tokens (    3.04 ms per token,   329.30 tokens per second)\n",
      "llama_print_timings:        eval time =      67.34 ms /     2 runs   (   33.67 ms per token,    29.70 tokens per second)\n",
      "llama_print_timings:       total time =     297.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     239.80 ms /    88 tokens (    2.73 ms per token,   366.97 tokens per second)\n",
      "llama_print_timings:        eval time =      33.86 ms /     1 runs   (   33.86 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:       total time =     276.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /     6 runs   (    0.15 ms per token,  6696.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.26 ms /   107 tokens (    2.45 ms per token,   407.99 tokens per second)\n",
      "llama_print_timings:        eval time =     172.96 ms /     5 runs   (   34.59 ms per token,    28.91 tokens per second)\n",
      "llama_print_timings:       total time =     444.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     237.08 ms /    82 tokens (    2.89 ms per token,   345.88 tokens per second)\n",
      "llama_print_timings:        eval time =     168.07 ms /     5 runs   (   33.61 ms per token,    29.75 tokens per second)\n",
      "llama_print_timings:       total time =     414.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6651.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     228.34 ms /    79 tokens (    2.89 ms per token,   345.98 tokens per second)\n",
      "llama_print_timings:        eval time =      67.58 ms /     2 runs   (   33.79 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =     300.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6535.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.07 ms /   122 tokens (    2.25 ms per token,   443.52 tokens per second)\n",
      "llama_print_timings:        eval time =      36.00 ms /     1 runs   (   36.00 ms per token,    27.77 tokens per second)\n",
      "llama_print_timings:       total time =     314.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6833.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     251.90 ms /    93 tokens (    2.71 ms per token,   369.20 tokens per second)\n",
      "llama_print_timings:        eval time =      69.70 ms /     2 runs   (   34.85 ms per token,    28.69 tokens per second)\n",
      "llama_print_timings:       total time =     326.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6637.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     587.08 ms /   259 tokens (    2.27 ms per token,   441.17 tokens per second)\n",
      "llama_print_timings:        eval time =      74.54 ms /     2 runs   (   37.27 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =     666.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /     7 runs   (    0.14 ms per token,  7268.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     234.55 ms /    82 tokens (    2.86 ms per token,   349.60 tokens per second)\n",
      "llama_print_timings:        eval time =     201.75 ms /     6 runs   (   33.62 ms per token,    29.74 tokens per second)\n",
      "llama_print_timings:       total time =     447.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6578.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.25 ms /   122 tokens (    2.26 ms per token,   443.23 tokens per second)\n",
      "llama_print_timings:        eval time =      35.93 ms /     1 runs   (   35.93 ms per token,    27.83 tokens per second)\n",
      "llama_print_timings:       total time =     314.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.87 ms /   259 tokens (    2.23 ms per token,   448.20 tokens per second)\n",
      "llama_print_timings:        eval time =      74.43 ms /     2 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
      "llama_print_timings:       total time =     656.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6493.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.79 ms /   122 tokens (    2.26 ms per token,   442.36 tokens per second)\n",
      "llama_print_timings:        eval time =      35.82 ms /     1 runs   (   35.82 ms per token,    27.91 tokens per second)\n",
      "llama_print_timings:       total time =     314.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6535.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     384.58 ms /   182 tokens (    2.11 ms per token,   473.24 tokens per second)\n",
      "llama_print_timings:        eval time =      36.33 ms /     1 runs   (   36.33 ms per token,    27.53 tokens per second)\n",
      "llama_print_timings:       total time =     423.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     6 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     215.14 ms /     6 runs   (   35.86 ms per token,    27.89 tokens per second)\n",
      "llama_print_timings:       total time =     224.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.00 ms /   100 tokens (    2.62 ms per token,   381.67 tokens per second)\n",
      "llama_print_timings:        eval time =      34.35 ms /     1 runs   (   34.35 ms per token,    29.11 tokens per second)\n",
      "llama_print_timings:       total time =     299.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6787.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     576.91 ms /   259 tokens (    2.23 ms per token,   448.95 tokens per second)\n",
      "llama_print_timings:        eval time =      74.85 ms /     2 runs   (   37.43 ms per token,    26.72 tokens per second)\n",
      "llama_print_timings:       total time =     656.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     522.60 ms /   245 tokens (    2.13 ms per token,   468.81 tokens per second)\n",
      "llama_print_timings:        eval time =      37.19 ms /     1 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
      "llama_print_timings:       total time =     563.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.26 ms /     2 runs   (    0.13 ms per token,  7751.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.14 ms /    89 tokens (    2.73 ms per token,   366.04 tokens per second)\n",
      "llama_print_timings:        eval time =      33.92 ms /     1 runs   (   33.92 ms per token,    29.48 tokens per second)\n",
      "llama_print_timings:       total time =     280.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6578.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.03 ms /    70 tokens (    3.20 ms per token,   312.46 tokens per second)\n",
      "llama_print_timings:        eval time =      34.07 ms /     1 runs   (   34.07 ms per token,    29.35 tokens per second)\n",
      "llama_print_timings:       total time =     261.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    15 runs   (    0.15 ms per token,  6634.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.39 ms /   122 tokens (    2.25 ms per token,   444.62 tokens per second)\n",
      "llama_print_timings:        eval time =     499.56 ms /    14 runs   (   35.68 ms per token,    28.02 tokens per second)\n",
      "llama_print_timings:       total time =     797.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     337.73 ms /   147 tokens (    2.30 ms per token,   435.26 tokens per second)\n",
      "llama_print_timings:        eval time =      35.60 ms /     1 runs   (   35.60 ms per token,    28.09 tokens per second)\n",
      "llama_print_timings:       total time =     376.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.04 ms /    20 runs   (    0.15 ms per token,  6568.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.65 ms /   122 tokens (    2.25 ms per token,   444.21 tokens per second)\n",
      "llama_print_timings:        eval time =     676.89 ms /    19 runs   (   35.63 ms per token,    28.07 tokens per second)\n",
      "llama_print_timings:       total time =     984.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     3 runs   (    0.14 ms per token,  6912.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     163.17 ms /    47 tokens (    3.47 ms per token,   288.05 tokens per second)\n",
      "llama_print_timings:        eval time =      65.14 ms /     2 runs   (   32.57 ms per token,    30.70 tokens per second)\n",
      "llama_print_timings:       total time =     233.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     3 runs   (    0.15 ms per token,  6593.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.31 ms /    92 tokens (    2.72 ms per token,   367.55 tokens per second)\n",
      "llama_print_timings:        eval time =      69.45 ms /     2 runs   (   34.73 ms per token,    28.80 tokens per second)\n",
      "llama_print_timings:       total time =     324.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6514.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.49 ms /   122 tokens (    2.25 ms per token,   444.46 tokens per second)\n",
      "llama_print_timings:        eval time =      35.76 ms /     1 runs   (   35.76 ms per token,    27.97 tokens per second)\n",
      "llama_print_timings:       total time =     313.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     3 runs   (    0.14 ms per token,  7211.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.94 ms /    59 tokens (    3.07 ms per token,   326.07 tokens per second)\n",
      "llama_print_timings:        eval time =      67.25 ms /     2 runs   (   33.63 ms per token,    29.74 tokens per second)\n",
      "llama_print_timings:       total time =     252.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6787.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.75 ms /    64 tokens (    2.92 ms per token,   342.69 tokens per second)\n",
      "llama_print_timings:        eval time =      67.92 ms /     2 runs   (   33.96 ms per token,    29.45 tokens per second)\n",
      "llama_print_timings:       total time =     259.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6741.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     249.98 ms /    92 tokens (    2.72 ms per token,   368.02 tokens per second)\n",
      "llama_print_timings:        eval time =      69.09 ms /     2 runs   (   34.55 ms per token,    28.95 tokens per second)\n",
      "llama_print_timings:       total time =     323.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6644.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     334.96 ms /   143 tokens (    2.34 ms per token,   426.92 tokens per second)\n",
      "llama_print_timings:        eval time =      35.78 ms /     1 runs   (   35.78 ms per token,    27.95 tokens per second)\n",
      "llama_print_timings:       total time =     373.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    15 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.07 ms /   122 tokens (    2.25 ms per token,   443.53 tokens per second)\n",
      "llama_print_timings:        eval time =     497.16 ms /    14 runs   (   35.51 ms per token,    28.16 tokens per second)\n",
      "llama_print_timings:       total time =     795.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6514.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.62 ms /    90 tokens (    2.71 ms per token,   369.43 tokens per second)\n",
      "llama_print_timings:        eval time =      34.79 ms /     1 runs   (   34.79 ms per token,    28.74 tokens per second)\n",
      "llama_print_timings:       total time =     281.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     257.22 ms /   100 tokens (    2.57 ms per token,   388.78 tokens per second)\n",
      "llama_print_timings:        eval time =      34.41 ms /     1 runs   (   34.41 ms per token,    29.06 tokens per second)\n",
      "llama_print_timings:       total time =     294.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6472.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.29 ms /   119 tokens (    2.30 ms per token,   435.44 tokens per second)\n",
      "llama_print_timings:        eval time =      34.82 ms /     1 runs   (   34.82 ms per token,    28.72 tokens per second)\n",
      "llama_print_timings:       total time =     311.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6493.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.92 ms /   122 tokens (    2.25 ms per token,   443.77 tokens per second)\n",
      "llama_print_timings:        eval time =      35.79 ms /     1 runs   (   35.79 ms per token,    27.94 tokens per second)\n",
      "llama_print_timings:       total time =     314.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.46 ms /   116 tokens (    2.32 ms per token,   430.49 tokens per second)\n",
      "llama_print_timings:        eval time =      68.98 ms /     2 runs   (   34.49 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:       total time =     343.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6818.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     103.70 ms /     3 runs   (   34.57 ms per token,    28.93 tokens per second)\n",
      "llama_print_timings:       total time =     107.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6557.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     187.28 ms /    63 tokens (    2.97 ms per token,   336.40 tokens per second)\n",
      "llama_print_timings:        eval time =      33.11 ms /     1 runs   (   33.11 ms per token,    30.21 tokens per second)\n",
      "llama_print_timings:       total time =     223.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6472.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.25 ms /    88 tokens (    2.75 ms per token,   363.26 tokens per second)\n",
      "llama_print_timings:        eval time =      33.44 ms /     1 runs   (   33.44 ms per token,    29.90 tokens per second)\n",
      "llama_print_timings:       total time =     279.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6615.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     282.02 ms /   124 tokens (    2.27 ms per token,   439.69 tokens per second)\n",
      "llama_print_timings:        eval time =     178.57 ms /     5 runs   (   35.71 ms per token,    28.00 tokens per second)\n",
      "llama_print_timings:       total time =     470.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.30 ms /   104 tokens (    2.48 ms per token,   402.64 tokens per second)\n",
      "llama_print_timings:        eval time =      34.86 ms /     1 runs   (   34.86 ms per token,    28.69 tokens per second)\n",
      "llama_print_timings:       total time =     296.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6629.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.67 ms /   124 tokens (    2.26 ms per token,   441.80 tokens per second)\n",
      "llama_print_timings:        eval time =     178.50 ms /     5 runs   (   35.70 ms per token,    28.01 tokens per second)\n",
      "llama_print_timings:       total time =     468.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.33 ms /     2 runs   (    0.16 ms per token,  6097.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =      71.24 ms /     2 runs   (   35.62 ms per token,    28.07 tokens per second)\n",
      "llama_print_timings:       total time =      74.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.14 ms per token,  6920.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     153.04 ms /    42 tokens (    3.64 ms per token,   274.44 tokens per second)\n",
      "llama_print_timings:        eval time =      33.24 ms /     1 runs   (   33.24 ms per token,    30.08 tokens per second)\n",
      "llama_print_timings:       total time =     189.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6607.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.73 ms /   124 tokens (    2.26 ms per token,   441.70 tokens per second)\n",
      "llama_print_timings:        eval time =     177.80 ms /     5 runs   (   35.56 ms per token,    28.12 tokens per second)\n",
      "llama_print_timings:       total time =     468.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /    15 runs   (    0.15 ms per token,  6702.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     534.32 ms /    15 runs   (   35.62 ms per token,    28.07 tokens per second)\n",
      "llama_print_timings:       total time =     557.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =      71.52 ms /     2 runs   (   35.76 ms per token,    27.97 tokens per second)\n",
      "llama_print_timings:       total time =      74.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    15 runs   (    0.15 ms per token,  6738.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     533.05 ms /    15 runs   (   35.54 ms per token,    28.14 tokens per second)\n",
      "llama_print_timings:       total time =     557.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6644.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     436.28 ms /   194 tokens (    2.25 ms per token,   444.67 tokens per second)\n",
      "llama_print_timings:        eval time =      36.80 ms /     1 runs   (   36.80 ms per token,    27.17 tokens per second)\n",
      "llama_print_timings:       total time =     475.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     386.05 ms /   182 tokens (    2.12 ms per token,   471.44 tokens per second)\n",
      "llama_print_timings:        eval time =      35.70 ms /     1 runs   (   35.70 ms per token,    28.01 tokens per second)\n",
      "llama_print_timings:       total time =     424.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     6 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     348.81 ms /   150 tokens (    2.33 ms per token,   430.03 tokens per second)\n",
      "llama_print_timings:        eval time =     179.05 ms /     5 runs   (   35.81 ms per token,    27.92 tokens per second)\n",
      "llama_print_timings:       total time =     537.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6696.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     331.27 ms /   140 tokens (    2.37 ms per token,   422.62 tokens per second)\n",
      "llama_print_timings:        eval time =      71.30 ms /     2 runs   (   35.65 ms per token,    28.05 tokens per second)\n",
      "llama_print_timings:       total time =     406.82 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /     6 runs   (    0.15 ms per token,  6681.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     281.49 ms /   124 tokens (    2.27 ms per token,   440.52 tokens per second)\n",
      "llama_print_timings:        eval time =     179.44 ms /     5 runs   (   35.89 ms per token,    27.86 tokens per second)\n",
      "llama_print_timings:       total time =     470.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.94 ms /     6 runs   (    0.16 ms per token,  6369.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.85 ms /   122 tokens (    2.24 ms per token,   445.50 tokens per second)\n",
      "llama_print_timings:        eval time =     186.16 ms /     5 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =     469.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /     6 runs   (    0.15 ms per token,  6674.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     282.05 ms /   124 tokens (    2.27 ms per token,   439.64 tokens per second)\n",
      "llama_print_timings:        eval time =     177.30 ms /     5 runs   (   35.46 ms per token,    28.20 tokens per second)\n",
      "llama_print_timings:       total time =     468.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6637.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.49 ms /    58 tokens (    3.06 ms per token,   326.77 tokens per second)\n",
      "llama_print_timings:        eval time =      67.75 ms /     2 runs   (   33.87 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =     249.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    22 runs   (    0.15 ms per token,  6614.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     283.09 ms /   124 tokens (    2.28 ms per token,   438.03 tokens per second)\n",
      "llama_print_timings:        eval time =     747.53 ms /    21 runs   (   35.60 ms per token,    28.09 tokens per second)\n",
      "llama_print_timings:       total time =    1066.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     378.91 ms /   169 tokens (    2.24 ms per token,   446.02 tokens per second)\n",
      "llama_print_timings:        eval time =      36.07 ms /     1 runs   (   36.07 ms per token,    27.73 tokens per second)\n",
      "llama_print_timings:       total time =     417.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.31 ms /     2 runs   (    0.15 ms per token,  6514.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.03 ms /   122 tokens (    2.25 ms per token,   443.59 tokens per second)\n",
      "llama_print_timings:        eval time =      35.43 ms /     1 runs   (   35.43 ms per token,    28.22 tokens per second)\n",
      "llama_print_timings:       total time =     313.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6629.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.94 ms /   124 tokens (    2.27 ms per token,   441.38 tokens per second)\n",
      "llama_print_timings:        eval time =     177.94 ms /     5 runs   (   35.59 ms per token,    28.10 tokens per second)\n",
      "llama_print_timings:       total time =     468.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.50 ms /    10 runs   (    0.15 ms per token,  6657.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     356.07 ms /    10 runs   (   35.61 ms per token,    28.08 tokens per second)\n",
      "llama_print_timings:       total time =     371.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6688.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =      70.92 ms /     2 runs   (   35.46 ms per token,    28.20 tokens per second)\n",
      "llama_print_timings:       total time =      73.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    15 runs   (    0.15 ms per token,  6605.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     535.07 ms /    15 runs   (   35.67 ms per token,    28.03 tokens per second)\n",
      "llama_print_timings:       total time =     558.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6557.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     368.26 ms /   159 tokens (    2.32 ms per token,   431.76 tokens per second)\n",
      "llama_print_timings:        eval time =      37.10 ms /     1 runs   (   37.10 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =     408.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6578.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     370.18 ms /   162 tokens (    2.29 ms per token,   437.63 tokens per second)\n",
      "llama_print_timings:        eval time =      36.43 ms /     1 runs   (   36.43 ms per token,    27.45 tokens per second)\n",
      "llama_print_timings:       total time =     409.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     6 runs   (    0.15 ms per token,  6586.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     282.43 ms /   124 tokens (    2.28 ms per token,   439.05 tokens per second)\n",
      "llama_print_timings:        eval time =     178.23 ms /     5 runs   (   35.65 ms per token,    28.05 tokens per second)\n",
      "llama_print_timings:       total time =     470.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6726.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.15 ms /    92 tokens (    2.72 ms per token,   367.77 tokens per second)\n",
      "llama_print_timings:        eval time =      68.74 ms /     2 runs   (   34.37 ms per token,    29.09 tokens per second)\n",
      "llama_print_timings:       total time =     323.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    33 runs   (    0.15 ms per token,  6696.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     281.77 ms /   124 tokens (    2.27 ms per token,   440.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1138.37 ms /    32 runs   (   35.57 ms per token,    28.11 tokens per second)\n",
      "llama_print_timings:       total time =    1473.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    15 runs   (    0.15 ms per token,  6735.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     533.39 ms /    15 runs   (   35.56 ms per token,    28.12 tokens per second)\n",
      "llama_print_timings:       total time =     557.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.28 ms /    15 runs   (    0.15 ms per token,  6570.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     534.65 ms /    15 runs   (   35.64 ms per token,    28.06 tokens per second)\n",
      "llama_print_timings:       total time =     558.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6557.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.48 ms /   116 tokens (    2.31 ms per token,   432.07 tokens per second)\n",
      "llama_print_timings:        eval time =      35.02 ms /     1 runs   (   35.02 ms per token,    28.56 tokens per second)\n",
      "llama_print_timings:       total time =     306.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for tweet in tweets:\n",
    "    prompt1 = \"Read the following tweet inside brackets:\\n\"\n",
    "    prompt2 = \"Classify the readed tweet as to whether it mentions sports betting, answer with text just yes or no: \"\n",
    "    str_for_promp = ''\n",
    "    str_for_promp += prompt1 + '['+tweet+']' + '\\n' + prompt2\n",
    "    prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "    USER: {str_for_promp}\n",
    "\n",
    "    ASSISTANT:\n",
    "    '''\n",
    "    response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)\n",
    "    to_filter = response[\"choices\"][0][\"text\"]\n",
    "    match = re.search(r'ASSISTANT:\\s*(.*)', to_filter, re.IGNORECASE)\n",
    "    answer = match.group(1).strip() if match else \"\"\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"classificacoes_nao_informadas.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"w\") as file:\n",
    "    for item in answers:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
