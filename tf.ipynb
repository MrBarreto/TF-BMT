{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando os dados Originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = pd.read_excel('tweets1.xlsx')\n",
    "tweets2 = pd.read_excel('tweets2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_combined = pd.concat([tweets1, tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english = tweets_combined[tweets_combined['Tweet Language'] == 'English'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english['ID'] = range(1, len(tweets_english) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered = tweets_english[['ID', 'Tweet Content']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\\n\\nGet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\"It’ll be a tough night for Europe today.\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>\"In defeat or in victory, always say Alhamduli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>\"FAFC Genesis Edition ( This collection have m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Get ready for zabardast action on #25th Jan ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Tweet Content\n",
       "1   1  \"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\\n\\nGet...\n",
       "3   2  \"It’ll be a tough night for Europe today.\\n\\n#...\n",
       "4   3  \"In defeat or in victory, always say Alhamduli...\n",
       "5   4  \"FAFC Genesis Edition ( This collection have m...\n",
       "6   5  \"Get ready for zabardast action on #25th Jan ...."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered.to_csv('tweets_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando dados na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273d5e42caad430f98b216973883f98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", \n",
    "                                             torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do CSV usando cudf\n",
    "df = cudf.read_csv('tweets_filtered.csv')\n",
    "\n",
    "# Converter a coluna de tweets para uma lista no cudf (mantendo na GPU)\n",
    "tweets = df['Tweet Content'].head(10).to_arrow().to_pylist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweets(tweets, batch_size=3):\n",
    "    results = []\n",
    "    prompt = \"Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \"\n",
    "    \n",
    "    for i in range(0, len(tweets), batch_size):\n",
    "        batch = tweets[i:i + batch_size]\n",
    "        str_for_promp = ''\n",
    "        for k in range(len(batch)):\n",
    "            str_for_promp += batch[k] + '\\n'\n",
    "        batch_with_prompt = prompt + '\\n' + str_for_promp\n",
    "        print(batch_with_prompt)\n",
    "        inputs = tokenizer(batch_with_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        # Usar autocast para mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=20)  # Ajuste max_new_tokens conforme necessário\n",
    "        \n",
    "        results.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\n",
      "\n",
      "Get into the #FIFAWorldCup Final mode with none other than @iamsrk &amp; @WayneRooney on Dec 18, LIVE on #JioCinema &amp; #Sports18 📺📲\n",
      "\n",
      "#Qatar2022 #ARGFRA #WorldsGreatestShow #FIFAWConJioCinema #FIFAWConSports18 #Pathaan\"\n",
      "\"It’ll be a tough night for Europe today.\n",
      "\n",
      "#FranceVsMorocco @ #FIFAWorldCup.\n",
      "\n",
      "Islamîsts will either celebrate the win of Islàm or avenge the defeat of Islàm.\"\n",
      "\"In defeat or in victory, always say Alhamdulillah and keep thanking Allah. And surely, Allah is the best of planners 🇲🇦♥️ #FIFAWorldCup https://t.co/CLSicv9g0h\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"FAFC Genesis Edition ( This collection have maximum supply of 107 NFTs) : https://t.co/bjvgIJMyRm\n",
      "\n",
      "Full collection of 7,910 NFTs will be launched on 16’Dec’2022! ⚽️🥅\n",
      "\n",
      "Join Discord : https://t.co/0OgjPZWayc\n",
      "\n",
      "#FIFA #GOAT𓃵 #FIFA22 #FIFA22 #LeoMessi #Messi #FIFAWorldCup https://t.co/mbkNgYDe1X\"\n",
      "\"Get ready for zabardast action on #25th Jan ..#PathaanFirstDayFirstShow \n",
      "#PathaanTeaser #pathaan\"\n",
      "\"Morocco won against Spain, there were riots in Spain\n",
      "\n",
      "Morocco won against Portugal, there were riots in Portugal\n",
      "\n",
      "Morocco lost to France, there were riots in France\n",
      "\n",
      "Morocco wins or loses, there be riots. And the reasons are all religious.\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"Morocco 🇲🇦 can be PROUD of their World Cup #FIFAWorldCupQatar2022 👏👏👏\n",
      "\n",
      "The ATLAS LIONS 🇲🇦 took down Belgium, Spain and Portugal on their way to becoming the FIRST AFRICAN NATION to ever make the #FIFAWorldCup semifinal. ❤️👏\n",
      "\n",
      "An incredible show of resilience and hard work. 👊 https://t.co/ARE2AKF6XZ\"\n",
      "\"Name the religion behind this rioting in France 🇫🇷 post their victory in #FIFAWorldCup against #Morocco https://t.co/vZ2XdE6Emo\"\n",
      "\"“This crazy fanbase leaves everything for the Cup!” 🎶 #ARG\n",
      "\n",
      "#Qatar2022 | #FIFAWorldCup https://t.co/3yWdNFM6Lp\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"This Moroccan side will be remembered for making World Cup history. \n",
      "\n",
      "What a manager, what a team 👏 The final was just a step too far.\n",
      "\n",
      "#BBCWorldCup #FifaWorldCup https://t.co/AVIaGfJKV7\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = classify_tweets(tweets)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \\n\"Morocco 🇲🇦 can be PROUD of their World Cup #FIFAWorldCupQatar2022 👏👏👏\\n\\nThe ATLAS LIONS 🇲🇦 took down Belgium, Spain and Portugal on their way to becoming the FIRST AFRICAN NATION to ever make the #FIFAWorldCup semifinal. ❤️👏\\n\\nAn incredible show of resilience and hard work. 👊 https://t.co/ARE2AKF6XZ\"\\n\"Name the religion behind this rioting in France 🇫🇷 post their victory in #FIFAWorldCup against #Morocco https://t.co/vZ2XdE6Emo\"\\n\"“This crazy fanbase leaves everything for the Cup!” 🎶 #ARG\\n\\n#Qatar2022 | #FIFAWorldCup https://t.co/3yWdNFM6Lp\"\\n\"Lionel Messi has been named the 2022 #FIFAWorldC'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tempo total: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22f7c79663f41af810e44dd17739edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aaae562e274788a5866cb2c9502a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a551f5d5c9418192351812cc148bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda146185304dac8a72790a6685bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58a55350364e3f9ff9c7efbe382560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95461ec93974f8699a9fd47da463a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a7d8231d2148e8a1afa1e1073c94d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
