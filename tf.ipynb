{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando os dados Originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = pd.read_excel('tweets1.xlsx')\n",
    "tweets2 = pd.read_excel('tweets2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_combined = pd.concat([tweets1, tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english = tweets_combined[tweets_combined['Tweet Language'] == 'English'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english['ID'] = range(1, len(tweets_english) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered = tweets_english[['ID', 'Tweet Content']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\\n\\nGet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Itâ€™ll be a tough night for Europe today.\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>\"In defeat or in victory, always say Alhamduli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>\"FAFC Genesis Edition ( This collection have m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Get ready for zabardast action on #25th Jan ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Tweet Content\n",
       "1   1  \"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\\n\\nGet...\n",
       "3   2  \"Itâ€™ll be a tough night for Europe today.\\n\\n#...\n",
       "4   3  \"In defeat or in victory, always say Alhamduli...\n",
       "5   4  \"FAFC Genesis Edition ( This collection have m...\n",
       "6   5  \"Get ready for zabardast action on #25th Jan ...."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered.to_csv('tweets_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando dados na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273d5e42caad430f98b216973883f98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", \n",
    "                                             torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do CSV usando cudf\n",
    "df = cudf.read_csv('tweets_filtered.csv')\n",
    "\n",
    "# Converter a coluna de tweets para uma lista no cudf (mantendo na GPU)\n",
    "tweets = df['Tweet Content'].head(10).to_arrow().to_pylist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweets(tweets, batch_size=3):\n",
    "    results = []\n",
    "    prompt = \"Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \"\n",
    "    \n",
    "    for i in range(0, len(tweets), batch_size):\n",
    "        batch = tweets[i:i + batch_size]\n",
    "        str_for_promp = ''\n",
    "        for k in range(len(batch)):\n",
    "            str_for_promp += batch[k] + '\\n'\n",
    "        batch_with_prompt = prompt + '\\n' + str_for_promp\n",
    "        print(batch_with_prompt)\n",
    "        inputs = tokenizer(batch_with_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        # Usar autocast para mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=20)  # Ajuste max_new_tokens conforme necessÃ¡rio\n",
    "        \n",
    "        results.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"ğˆğ­. ğ‚ğšğ§ğ§ğ¨ğ­. ğ†ğğ­. ğğ¢ğ ğ ğğ«. ğ“ğ¡ğšğ§. ğ“ğ¡ğ¢ğ¬. ğŸ”¥\n",
      "\n",
      "Get into the #FIFAWorldCup Final mode with none other than @iamsrk &amp; @WayneRooney on Dec 18, LIVE on #JioCinema &amp; #Sports18 ğŸ“ºğŸ“²\n",
      "\n",
      "#Qatar2022 #ARGFRA #WorldsGreatestShow #FIFAWConJioCinema #FIFAWConSports18 #Pathaan\"\n",
      "\"Itâ€™ll be a tough night for Europe today.\n",
      "\n",
      "#FranceVsMorocco @ #FIFAWorldCup.\n",
      "\n",
      "IslamÃ®sts will either celebrate the win of IslÃ m or avenge the defeat of IslÃ m.\"\n",
      "\"In defeat or in victory, always say Alhamdulillah and keep thanking Allah. And surely, Allah is the best of planners ğŸ‡²ğŸ‡¦â™¥ï¸ #FIFAWorldCup https://t.co/CLSicv9g0h\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"FAFC Genesis Edition ( This collection have maximum supply of 107 NFTs) : https://t.co/bjvgIJMyRm\n",
      "\n",
      "Full collection of 7,910 NFTs will be launched on 16â€™Decâ€™2022! âš½ï¸ğŸ¥…\n",
      "\n",
      "Join Discord : https://t.co/0OgjPZWayc\n",
      "\n",
      "#FIFA #GOATğ“ƒµ #FIFA22 #FIFA22 #LeoMessi #Messi #FIFAWorldCup https://t.co/mbkNgYDe1X\"\n",
      "\"Get ready for zabardast action on #25th Jan ..#PathaanFirstDayFirstShow \n",
      "#PathaanTeaser #pathaan\"\n",
      "\"Morocco won against Spain, there were riots in Spain\n",
      "\n",
      "Morocco won against Portugal, there were riots in Portugal\n",
      "\n",
      "Morocco lost to France, there were riots in France\n",
      "\n",
      "Morocco wins or loses, there be riots. And the reasons are all religious.\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"Morocco ğŸ‡²ğŸ‡¦ can be PROUD of their World Cup #FIFAWorldCupQatar2022 ğŸ‘ğŸ‘ğŸ‘\n",
      "\n",
      "The ATLAS LIONS ğŸ‡²ğŸ‡¦ took down Belgium, Spain and Portugal on their way to becoming the FIRST AFRICAN NATION to ever make the #FIFAWorldCup semifinal. â¤ï¸ğŸ‘\n",
      "\n",
      "An incredible show of resilience and hard work. ğŸ‘Š https://t.co/ARE2AKF6XZ\"\n",
      "\"Name the religion behind this rioting in France ğŸ‡«ğŸ‡· post their victory in #FIFAWorldCup against #Morocco https://t.co/vZ2XdE6Emo\"\n",
      "\"â€œThis crazy fanbase leaves everything for the Cup!â€ ğŸ¶ #ARG\n",
      "\n",
      "#Qatar2022 | #FIFAWorldCup https://t.co/3yWdNFM6Lp\"\n",
      "\n",
      "Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \n",
      "\"This Moroccan side will be remembered for making World Cup history. \n",
      "\n",
      "What a manager, what a team ğŸ‘ The final was just a step too far.\n",
      "\n",
      "#BBCWorldCup #FifaWorldCup https://t.co/AVIaGfJKV7\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = classify_tweets(tweets)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classify the following tweets as to whether it mentions sports betting, answer with yes or no, one per line: \\n\"Morocco ğŸ‡²ğŸ‡¦ can be PROUD of their World Cup #FIFAWorldCupQatar2022 ğŸ‘ğŸ‘ğŸ‘\\n\\nThe ATLAS LIONS ğŸ‡²ğŸ‡¦ took down Belgium, Spain and Portugal on their way to becoming the FIRST AFRICAN NATION to ever make the #FIFAWorldCup semifinal. â¤ï¸ğŸ‘\\n\\nAn incredible show of resilience and hard work. ğŸ‘Š https://t.co/ARE2AKF6XZ\"\\n\"Name the religion behind this rioting in France ğŸ‡«ğŸ‡· post their victory in #FIFAWorldCup against #Morocco https://t.co/vZ2XdE6Emo\"\\n\"â€œThis crazy fanbase leaves everything for the Cup!â€ ğŸ¶ #ARG\\n\\n#Qatar2022 | #FIFAWorldCup https://t.co/3yWdNFM6Lp\"\\n\"Lionel Messi has been named the 2022 #FIFAWorldC'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tempo total: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22f7c79663f41af810e44dd17739edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aaae562e274788a5866cb2c9502a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a551f5d5c9418192351812cc148bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda146185304dac8a72790a6685bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58a55350364e3f9ff9c7efbe382560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95461ec93974f8699a9fd47da463a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a7d8231d2148e8a1afa1e1073c94d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
