{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrando os dados Originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = pd.read_excel('tweets1.xlsx')\n",
    "tweets2 = pd.read_excel('tweets2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_combined = pd.concat([tweets1, tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english = tweets_combined[tweets_combined['Tweet Language'] == 'English'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_english['ID'] = range(1, len(tweets_english) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered = tweets_english[['ID', 'Tweet Content']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\\n\\nGet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\"It’ll be a tough night for Europe today.\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>\"In defeat or in victory, always say Alhamduli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>\"FAFC Genesis Edition ( This collection have m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Get ready for zabardast action on #25th Jan ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                      Tweet Content\n",
       "1   1  \"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\\n\\nGet...\n",
       "3   2  \"It’ll be a tough night for Europe today.\\n\\n#...\n",
       "4   3  \"In defeat or in victory, always say Alhamduli...\n",
       "5   4  \"FAFC Genesis Edition ( This collection have m...\n",
       "6   5  \"Get ready for zabardast action on #25th Jan ...."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtered.to_csv('tweets_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando dados na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do CSV usando cudf\n",
    "df = cudf.read_csv('tweets_filtered.csv')\n",
    "\n",
    "# Converter a coluna de tweets para uma lista no cudf (mantendo na GPU)\n",
    "tweets = df['Tweet Content'].head(100).to_arrow().to_pylist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22f7c79663f41af810e44dd17739edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aaae562e274788a5866cb2c9502a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a551f5d5c9418192351812cc148bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda146185304dac8a72790a6685bc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58a55350364e3f9ff9c7efbe382560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95461ec93974f8699a9fd47da463a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a7d8231d2148e8a1afa1e1073c94d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python --prefer-binary --no-cache-dir --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=4,  # Número de núcleos da CPU\n",
    "    n_batch=512,  # Deve estar entre 1 e n_ctx, considere a quantidade de VRAM na sua GPU\n",
    "    n_gpu_layers=32  # Ajuste com base no modelo e na VRAM da GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcpp_llm.model_params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Read the following tweet inside brackets:\\n\"\n",
    "prompt2 = \"Classify the readed tweet as to whether it mentions sports betting, answer just yes or no.: \"\n",
    "str_for_promp = ''\n",
    "str_for_promp += prompt1 + '['+tweets[0]+']' + '\\n' + prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {str_for_promp}\n",
    "\n",
    "ASSISTANT:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6864.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.82 ms /     9 tokens (   11.42 ms per token,    87.53 tokens per second)\n",
      "llama_print_timings:        eval time =      79.73 ms /     2 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
      "llama_print_timings:       total time =     193.95 ms\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Read the following tweet inside brackets:\n",
      "[\"𝐈𝐭. 𝐂𝐚𝐧𝐧𝐨𝐭. 𝐆𝐞𝐭. 𝐁𝐢𝐠𝐠𝐞𝐫. 𝐓𝐡𝐚𝐧. 𝐓𝐡𝐢𝐬. 🔥\n",
      "\n",
      "Get into the #FIFAWorldCup Final mode with none other than @iamsrk &amp; @WayneRooney on Dec 18, LIVE on #JioCinema &amp; #Sports18 📺📲\n",
      "\n",
      "#Qatar2022 #ARGFRA #WorldsGreatestShow #FIFAWConJioCinema #FIFAWConSports18 #Pathaan\"]\n",
      "Classify the readed tweet as to whether it mentions sports betting, answer just yes or no.: \n",
      "\n",
      "ASSISTANT:\n",
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'ASSISTANT:\\s*(yes|no)', generated_text, re.IGNORECASE)\n",
    "answer = match.group(1).lower() if match else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    15 runs   (    0.15 ms per token,  6796.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     616.50 ms /   271 tokens (    2.27 ms per token,   439.58 tokens per second)\n",
      "llama_print_timings:        eval time =     517.29 ms /    14 runs   (   36.95 ms per token,    27.06 tokens per second)\n",
      "llama_print_timings:       total time =    1157.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     3 runs   (    0.14 ms per token,  6960.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     255.95 ms /    93 tokens (    2.75 ms per token,   363.35 tokens per second)\n",
      "llama_print_timings:        eval time =      69.43 ms /     2 runs   (   34.72 ms per token,    28.81 tokens per second)\n",
      "llama_print_timings:       total time =     330.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.58 ms /    11 runs   (    0.14 ms per token,  6966.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     253.73 ms /    99 tokens (    2.56 ms per token,   390.18 tokens per second)\n",
      "llama_print_timings:        eval time =     347.37 ms /    10 runs   (   34.74 ms per token,    28.79 tokens per second)\n",
      "llama_print_timings:       total time =     618.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.95 ms /   180 tokens (    2.17 ms per token,   460.41 tokens per second)\n",
      "llama_print_timings:        eval time =      36.00 ms /     1 runs   (   36.00 ms per token,    27.78 tokens per second)\n",
      "llama_print_timings:       total time =     429.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     3 runs   (    0.14 ms per token,  7142.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.16 ms /    69 tokens (    3.23 ms per token,   309.20 tokens per second)\n",
      "llama_print_timings:        eval time =      67.31 ms /     2 runs   (   33.66 ms per token,    29.71 tokens per second)\n",
      "llama_print_timings:       total time =     294.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.14 ms per token,  6944.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.32 ms /   100 tokens (    2.62 ms per token,   381.22 tokens per second)\n",
      "llama_print_timings:        eval time =      34.73 ms /     1 runs   (   34.73 ms per token,    28.80 tokens per second)\n",
      "llama_print_timings:       total time =     299.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     378.04 ms /   172 tokens (    2.20 ms per token,   454.97 tokens per second)\n",
      "llama_print_timings:        eval time =      35.97 ms /     1 runs   (   35.97 ms per token,    27.80 tokens per second)\n",
      "llama_print_timings:       total time =     416.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.58 ms /    24 runs   (    0.15 ms per token,  6703.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     234.99 ms /    83 tokens (    2.83 ms per token,   353.21 tokens per second)\n",
      "llama_print_timings:        eval time =     786.48 ms /    23 runs   (   34.19 ms per token,    29.24 tokens per second)\n",
      "llama_print_timings:       total time =    1061.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.12 ms /    15 runs   (    0.14 ms per token,  7088.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     241.00 ms /    86 tokens (    2.80 ms per token,   356.85 tokens per second)\n",
      "llama_print_timings:        eval time =     474.84 ms /    14 runs   (   33.92 ms per token,    29.48 tokens per second)\n",
      "llama_print_timings:       total time =     738.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    16 runs   (    0.15 ms per token,  6849.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     260.90 ms /   102 tokens (    2.56 ms per token,   390.95 tokens per second)\n",
      "llama_print_timings:        eval time =     518.41 ms /    15 runs   (   34.56 ms per token,    28.93 tokens per second)\n",
      "llama_print_timings:       total time =     804.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    15 runs   (    0.15 ms per token,  6880.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     579.77 ms /   257 tokens (    2.26 ms per token,   443.28 tokens per second)\n",
      "llama_print_timings:        eval time =     522.81 ms /    14 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
      "llama_print_timings:       total time =    1125.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.63 ms /    11 runs   (    0.15 ms per token,  6736.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     221.45 ms /    68 tokens (    3.26 ms per token,   307.07 tokens per second)\n",
      "llama_print_timings:        eval time =     336.89 ms /    10 runs   (   33.69 ms per token,    29.68 tokens per second)\n",
      "llama_print_timings:       total time =     576.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6880.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.76 ms /    90 tokens (    2.71 ms per token,   369.22 tokens per second)\n",
      "llama_print_timings:        eval time =      69.31 ms /     2 runs   (   34.66 ms per token,    28.85 tokens per second)\n",
      "llama_print_timings:       total time =     317.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     3 runs   (    0.15 ms per token,  6564.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     338.50 ms /   147 tokens (    2.30 ms per token,   434.26 tokens per second)\n",
      "llama_print_timings:        eval time =      71.45 ms /     2 runs   (   35.72 ms per token,    27.99 tokens per second)\n",
      "llama_print_timings:       total time =     414.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6872.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     227.16 ms /    74 tokens (    3.07 ms per token,   325.76 tokens per second)\n",
      "llama_print_timings:        eval time =      34.12 ms /     1 runs   (   34.12 ms per token,    29.31 tokens per second)\n",
      "llama_print_timings:       total time =     264.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    15 runs   (    0.15 ms per token,  6824.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.40 ms /   108 tokens (    2.44 ms per token,   410.03 tokens per second)\n",
      "llama_print_timings:        eval time =     481.67 ms /    14 runs   (   34.40 ms per token,    29.07 tokens per second)\n",
      "llama_print_timings:       total time =     769.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.42 ms /    24 runs   (    0.14 ms per token,  7027.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.89 ms /   114 tokens (    2.36 ms per token,   423.96 tokens per second)\n",
      "llama_print_timings:        eval time =     811.38 ms /    23 runs   (   35.28 ms per token,    28.35 tokens per second)\n",
      "llama_print_timings:       total time =    1119.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6578.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     287.61 ms /   120 tokens (    2.40 ms per token,   417.23 tokens per second)\n",
      "llama_print_timings:        eval time =      34.96 ms /     1 runs   (   34.96 ms per token,    28.61 tokens per second)\n",
      "llama_print_timings:       total time =     326.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.58 ms /    11 runs   (    0.14 ms per token,  6962.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     343.21 ms /   151 tokens (    2.27 ms per token,   439.97 tokens per second)\n",
      "llama_print_timings:        eval time =     358.08 ms /    10 runs   (   35.81 ms per token,    27.93 tokens per second)\n",
      "llama_print_timings:       total time =     718.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.57 ms /    11 runs   (    0.14 ms per token,  6997.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     405.42 ms /   179 tokens (    2.26 ms per token,   441.52 tokens per second)\n",
      "llama_print_timings:        eval time =     359.55 ms /    10 runs   (   35.96 ms per token,    27.81 tokens per second)\n",
      "llama_print_timings:       total time =     781.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    16 runs   (    0.15 ms per token,  6831.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.33 ms /   257 tokens (    2.25 ms per token,   445.15 tokens per second)\n",
      "llama_print_timings:        eval time =     558.38 ms /    15 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_print_timings:       total time =    1161.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6734.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     362.62 ms /   151 tokens (    2.40 ms per token,   416.42 tokens per second)\n",
      "llama_print_timings:        eval time =      36.02 ms /     1 runs   (   36.02 ms per token,    27.76 tokens per second)\n",
      "llama_print_timings:       total time =     401.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.32 ms /    17 runs   (    0.14 ms per token,  7337.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     225.16 ms /    72 tokens (    3.13 ms per token,   319.77 tokens per second)\n",
      "llama_print_timings:        eval time =     536.45 ms /    16 runs   (   33.53 ms per token,    29.83 tokens per second)\n",
      "llama_print_timings:       total time =     787.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6734.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     238.90 ms /    86 tokens (    2.78 ms per token,   359.98 tokens per second)\n",
      "llama_print_timings:        eval time =      33.80 ms /     1 runs   (   33.80 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =     275.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6779.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.71 ms /   105 tokens (    2.54 ms per token,   393.68 tokens per second)\n",
      "llama_print_timings:        eval time =      34.34 ms /     1 runs   (   34.34 ms per token,    29.12 tokens per second)\n",
      "llama_print_timings:       total time =     303.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6825.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     232.46 ms /    80 tokens (    2.91 ms per token,   344.14 tokens per second)\n",
      "llama_print_timings:        eval time =      34.29 ms /     1 runs   (   34.29 ms per token,    29.17 tokens per second)\n",
      "llama_print_timings:       total time =     269.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6741.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     230.20 ms /    77 tokens (    2.99 ms per token,   334.50 tokens per second)\n",
      "llama_print_timings:        eval time =      67.57 ms /     2 runs   (   33.79 ms per token,    29.60 tokens per second)\n",
      "llama_print_timings:       total time =     302.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6711.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.57 ms /   120 tokens (    2.29 ms per token,   437.04 tokens per second)\n",
      "llama_print_timings:        eval time =      34.62 ms /     1 runs   (   34.62 ms per token,    28.88 tokens per second)\n",
      "llama_print_timings:       total time =     311.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.48 ms /    24 runs   (    0.14 ms per token,  6904.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.67 ms /    91 tokens (    2.75 ms per token,   363.02 tokens per second)\n",
      "llama_print_timings:        eval time =     792.13 ms /    23 runs   (   34.44 ms per token,    29.04 tokens per second)\n",
      "llama_print_timings:       total time =    1080.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.30 ms /    15 runs   (    0.15 ms per token,  6513.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     575.24 ms /   257 tokens (    2.24 ms per token,   446.77 tokens per second)\n",
      "llama_print_timings:        eval time =     519.22 ms /    14 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
      "llama_print_timings:       total time =    1119.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.95 ms /     7 runs   (    0.14 ms per token,  7383.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     232.33 ms /    80 tokens (    2.90 ms per token,   344.34 tokens per second)\n",
      "llama_print_timings:        eval time =     200.41 ms /     6 runs   (   33.40 ms per token,    29.94 tokens per second)\n",
      "llama_print_timings:       total time =     442.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.58 ms /    11 runs   (    0.14 ms per token,  6944.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.71 ms /   120 tokens (    2.29 ms per token,   436.83 tokens per second)\n",
      "llama_print_timings:        eval time =     351.02 ms /    10 runs   (   35.10 ms per token,    28.49 tokens per second)\n",
      "llama_print_timings:       total time =     642.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.01 ms /     7 runs   (    0.14 ms per token,  6910.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     568.97 ms /   257 tokens (    2.21 ms per token,   451.69 tokens per second)\n",
      "llama_print_timings:        eval time =     221.89 ms /     6 runs   (   36.98 ms per token,    27.04 tokens per second)\n",
      "llama_print_timings:       total time =     801.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6644.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.01 ms /   120 tokens (    2.28 ms per token,   437.93 tokens per second)\n",
      "llama_print_timings:        eval time =      34.44 ms /     1 runs   (   34.44 ms per token,    29.04 tokens per second)\n",
      "llama_print_timings:       total time =     311.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6779.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     377.81 ms /   180 tokens (    2.10 ms per token,   476.43 tokens per second)\n",
      "llama_print_timings:        eval time =      35.54 ms /     1 runs   (   35.54 ms per token,    28.13 tokens per second)\n",
      "llama_print_timings:       total time =     415.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.22 ms /    15 runs   (    0.15 ms per token,  6759.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     548.24 ms /    15 runs   (   36.55 ms per token,    27.36 tokens per second)\n",
      "llama_print_timings:       total time =     572.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.73 ms /    98 tokens (    2.72 ms per token,   367.41 tokens per second)\n",
      "llama_print_timings:        eval time =      34.86 ms /     1 runs   (   34.86 ms per token,    28.68 tokens per second)\n",
      "llama_print_timings:       total time =     304.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    15 runs   (    0.15 ms per token,  6811.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     584.58 ms /   257 tokens (    2.27 ms per token,   439.63 tokens per second)\n",
      "llama_print_timings:        eval time =     529.17 ms /    14 runs   (   37.80 ms per token,    26.46 tokens per second)\n",
      "llama_print_timings:       total time =    1138.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       4.24 ms /    29 runs   (    0.15 ms per token,  6847.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     527.87 ms /   243 tokens (    2.17 ms per token,   460.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1053.26 ms /    28 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
      "llama_print_timings:       total time =    1630.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.26 ms /     2 runs   (    0.13 ms per token,  7812.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     254.89 ms /    87 tokens (    2.93 ms per token,   341.33 tokens per second)\n",
      "llama_print_timings:        eval time =      34.80 ms /     1 runs   (   34.80 ms per token,    28.73 tokens per second)\n",
      "llama_print_timings:       total time =     293.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     6 runs   (    0.15 ms per token,  6779.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     226.30 ms /    68 tokens (    3.33 ms per token,   300.49 tokens per second)\n",
      "llama_print_timings:        eval time =     169.05 ms /     5 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =     404.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.60 ms /    11 runs   (    0.15 ms per token,  6870.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.83 ms /   120 tokens (    2.32 ms per token,   431.92 tokens per second)\n",
      "llama_print_timings:        eval time =     353.31 ms /    10 runs   (   35.33 ms per token,    28.30 tokens per second)\n",
      "llama_print_timings:       total time =     648.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    15 runs   (    0.15 ms per token,  6868.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     336.94 ms /   145 tokens (    2.32 ms per token,   430.34 tokens per second)\n",
      "llama_print_timings:        eval time =     503.22 ms /    14 runs   (   35.94 ms per token,    27.82 tokens per second)\n",
      "llama_print_timings:       total time =     864.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.60 ms /    11 runs   (    0.15 ms per token,  6870.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     295.71 ms /   120 tokens (    2.46 ms per token,   405.81 tokens per second)\n",
      "llama_print_timings:        eval time =     353.50 ms /    10 runs   (   35.35 ms per token,    28.29 tokens per second)\n",
      "llama_print_timings:       total time =     666.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.14 ms per token,  7017.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.59 ms /    45 tokens (    3.84 ms per token,   260.74 tokens per second)\n",
      "llama_print_timings:        eval time =      32.93 ms /     1 runs   (   32.93 ms per token,    30.37 tokens per second)\n",
      "llama_print_timings:       total time =     208.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6864.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     247.91 ms /    90 tokens (    2.75 ms per token,   363.04 tokens per second)\n",
      "llama_print_timings:        eval time =      69.08 ms /     2 runs   (   34.54 ms per token,    28.95 tokens per second)\n",
      "llama_print_timings:       total time =     321.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.73 ms /   120 tokens (    2.34 ms per token,   427.46 tokens per second)\n",
      "llama_print_timings:        eval time =      34.49 ms /     1 runs   (   34.49 ms per token,    28.99 tokens per second)\n",
      "llama_print_timings:       total time =     318.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /     7 runs   (    0.14 ms per token,  7268.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     177.62 ms /    57 tokens (    3.12 ms per token,   320.91 tokens per second)\n",
      "llama_print_timings:        eval time =     201.98 ms /     6 runs   (   33.66 ms per token,    29.71 tokens per second)\n",
      "llama_print_timings:       total time =     391.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6880.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     196.76 ms /    62 tokens (    3.17 ms per token,   315.11 tokens per second)\n",
      "llama_print_timings:        eval time =      67.20 ms /     2 runs   (   33.60 ms per token,    29.76 tokens per second)\n",
      "llama_print_timings:       total time =     268.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     3 runs   (    0.14 ms per token,  6912.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     256.98 ms /    90 tokens (    2.86 ms per token,   350.23 tokens per second)\n",
      "llama_print_timings:        eval time =      69.24 ms /     2 runs   (   34.62 ms per token,    28.88 tokens per second)\n",
      "llama_print_timings:       total time =     330.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.15 ms per token,  6849.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     345.61 ms /   141 tokens (    2.45 ms per token,   407.97 tokens per second)\n",
      "llama_print_timings:        eval time =      35.19 ms /     1 runs   (   35.19 ms per token,    28.42 tokens per second)\n",
      "llama_print_timings:       total time =     383.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.50 ms /    24 runs   (    0.15 ms per token,  6864.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.70 ms /   120 tokens (    2.33 ms per token,   429.03 tokens per second)\n",
      "llama_print_timings:        eval time =     815.62 ms /    23 runs   (   35.46 ms per token,    28.20 tokens per second)\n",
      "llama_print_timings:       total time =    1134.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     2 runs   (    0.14 ms per token,  6968.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     245.82 ms /    88 tokens (    2.79 ms per token,   357.98 tokens per second)\n",
      "llama_print_timings:        eval time =      33.67 ms /     1 runs   (   33.67 ms per token,    29.70 tokens per second)\n",
      "llama_print_timings:       total time =     282.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    15 runs   (    0.15 ms per token,  6672.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.91 ms /    98 tokens (    2.68 ms per token,   372.75 tokens per second)\n",
      "llama_print_timings:        eval time =     489.32 ms /    14 runs   (   34.95 ms per token,    28.61 tokens per second)\n",
      "llama_print_timings:       total time =     778.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /    11 runs   (    0.15 ms per token,  6836.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.52 ms /   117 tokens (    2.32 ms per token,   430.91 tokens per second)\n",
      "llama_print_timings:        eval time =     351.28 ms /    10 runs   (   35.13 ms per token,    28.47 tokens per second)\n",
      "llama_print_timings:       total time =     640.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6622.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.08 ms /   120 tokens (    2.32 ms per token,   431.53 tokens per second)\n",
      "llama_print_timings:        eval time =      34.23 ms /     1 runs   (   34.23 ms per token,    29.21 tokens per second)\n",
      "llama_print_timings:       total time =     315.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     3 runs   (    0.15 ms per token,  6741.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.24 ms /   114 tokens (    2.35 ms per token,   425.00 tokens per second)\n",
      "llama_print_timings:        eval time =      68.57 ms /     2 runs   (   34.28 ms per token,    29.17 tokens per second)\n",
      "llama_print_timings:       total time =     341.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6880.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     103.15 ms /     3 runs   (   34.38 ms per token,    29.08 tokens per second)\n",
      "llama_print_timings:       total time =     107.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.57 ms /    18 runs   (    0.14 ms per token,  7014.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.45 ms /    61 tokens (    3.30 ms per token,   302.80 tokens per second)\n",
      "llama_print_timings:        eval time =     574.90 ms /    17 runs   (   33.82 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:       total time =     805.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6688.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     237.36 ms /    86 tokens (    2.76 ms per token,   362.31 tokens per second)\n",
      "llama_print_timings:        eval time =      33.37 ms /     1 runs   (   33.37 ms per token,    29.96 tokens per second)\n",
      "llama_print_timings:       total time =     273.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    15 runs   (    0.15 ms per token,  6896.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.69 ms /   122 tokens (    2.27 ms per token,   440.93 tokens per second)\n",
      "llama_print_timings:        eval time =     494.99 ms /    14 runs   (   35.36 ms per token,    28.28 tokens per second)\n",
      "llama_print_timings:       total time =     796.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =      10.12 ms /    68 runs   (    0.15 ms per token,  6718.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.13 ms /   102 tokens (    2.58 ms per token,   387.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2407.88 ms /    67 runs   (   35.94 ms per token,    27.83 tokens per second)\n",
      "llama_print_timings:       total time =    2787.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    15 runs   (    0.15 ms per token,  6651.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.73 ms /   122 tokens (    2.28 ms per token,   439.28 tokens per second)\n",
      "llama_print_timings:        eval time =     498.38 ms /    14 runs   (   35.60 ms per token,    28.09 tokens per second)\n",
      "llama_print_timings:       total time =     801.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    36 runs   (    0.15 ms per token,  6878.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    1276.30 ms /    36 runs   (   35.45 ms per token,    28.21 tokens per second)\n",
      "llama_print_timings:       total time =    1334.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.28 ms /     2 runs   (    0.14 ms per token,  7220.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     154.34 ms /    40 tokens (    3.86 ms per token,   259.16 tokens per second)\n",
      "llama_print_timings:        eval time =      31.85 ms /     1 runs   (   31.85 ms per token,    31.40 tokens per second)\n",
      "llama_print_timings:       total time =     188.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.88 ms /     6 runs   (    0.15 ms per token,  6841.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.95 ms /   122 tokens (    2.26 ms per token,   442.10 tokens per second)\n",
      "llama_print_timings:        eval time =     177.15 ms /     5 runs   (   35.43 ms per token,    28.22 tokens per second)\n",
      "llama_print_timings:       total time =     462.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /    16 runs   (    0.15 ms per token,  6872.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     564.42 ms /    16 runs   (   35.28 ms per token,    28.35 tokens per second)\n",
      "llama_print_timings:       total time =     590.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.38 ms /    23 runs   (    0.15 ms per token,  6806.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     813.42 ms /    23 runs   (   35.37 ms per token,    28.28 tokens per second)\n",
      "llama_print_timings:       total time =     850.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    15 runs   (    0.15 ms per token,  6843.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     531.05 ms /    15 runs   (   35.40 ms per token,    28.25 tokens per second)\n",
      "llama_print_timings:       total time =     555.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6734.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     410.47 ms /   192 tokens (    2.14 ms per token,   467.76 tokens per second)\n",
      "llama_print_timings:        eval time =      37.02 ms /     1 runs   (   37.02 ms per token,    27.01 tokens per second)\n",
      "llama_print_timings:       total time =     450.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    15 runs   (    0.15 ms per token,  6815.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     390.38 ms /   180 tokens (    2.17 ms per token,   461.10 tokens per second)\n",
      "llama_print_timings:        eval time =     512.12 ms /    14 runs   (   36.58 ms per token,    27.34 tokens per second)\n",
      "llama_print_timings:       total time =     927.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /    15 runs   (    0.15 ms per token,  6684.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     346.61 ms /   148 tokens (    2.34 ms per token,   426.99 tokens per second)\n",
      "llama_print_timings:        eval time =     499.81 ms /    14 runs   (   35.70 ms per token,    28.01 tokens per second)\n",
      "llama_print_timings:       total time =     872.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.10 ms /    15 runs   (    0.14 ms per token,  7159.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     329.85 ms /   138 tokens (    2.39 ms per token,   418.38 tokens per second)\n",
      "llama_print_timings:        eval time =     495.33 ms /    14 runs   (   35.38 ms per token,    28.26 tokens per second)\n",
      "llama_print_timings:       total time =     849.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.36 ms /    23 runs   (    0.15 ms per token,  6851.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.05 ms /   122 tokens (    2.25 ms per token,   445.17 tokens per second)\n",
      "llama_print_timings:        eval time =     777.14 ms /    22 runs   (   35.32 ms per token,    28.31 tokens per second)\n",
      "llama_print_timings:       total time =    1089.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.59 ms /    11 runs   (    0.14 ms per token,  6918.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.29 ms /   120 tokens (    2.28 ms per token,   439.09 tokens per second)\n",
      "llama_print_timings:        eval time =     351.17 ms /    10 runs   (   35.12 ms per token,    28.48 tokens per second)\n",
      "llama_print_timings:       total time =     642.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    35 runs   (    0.15 ms per token,  6870.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.85 ms /   122 tokens (    2.24 ms per token,   445.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1206.28 ms /    34 runs   (   35.48 ms per token,    28.19 tokens per second)\n",
      "llama_print_timings:       total time =    1538.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     3 runs   (    0.15 ms per token,  6593.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     172.80 ms /    56 tokens (    3.09 ms per token,   324.08 tokens per second)\n",
      "llama_print_timings:        eval time =      65.19 ms /     2 runs   (   32.60 ms per token,    30.68 tokens per second)\n",
      "llama_print_timings:       total time =     243.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.87 ms /     6 runs   (    0.15 ms per token,  6872.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.75 ms /   122 tokens (    2.26 ms per token,   442.44 tokens per second)\n",
      "llama_print_timings:        eval time =     176.48 ms /     5 runs   (   35.30 ms per token,    28.33 tokens per second)\n",
      "llama_print_timings:       total time =     461.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    15 runs   (    0.14 ms per token,  6915.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     372.77 ms /   167 tokens (    2.23 ms per token,   448.00 tokens per second)\n",
      "llama_print_timings:        eval time =     507.50 ms /    14 runs   (   36.25 ms per token,    27.59 tokens per second)\n",
      "llama_print_timings:       total time =     906.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.13 ms /   120 tokens (    2.33 ms per token,   428.37 tokens per second)\n",
      "llama_print_timings:        eval time =      34.33 ms /     1 runs   (   34.33 ms per token,    29.13 tokens per second)\n",
      "llama_print_timings:       total time =     317.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    15 runs   (    0.14 ms per token,  6899.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.40 ms /   122 tokens (    2.26 ms per token,   442.99 tokens per second)\n",
      "llama_print_timings:        eval time =     498.36 ms /    14 runs   (   35.60 ms per token,    28.09 tokens per second)\n",
      "llama_print_timings:       total time =     799.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    15 runs   (    0.15 ms per token,  6808.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     539.53 ms /    15 runs   (   35.97 ms per token,    27.80 tokens per second)\n",
      "llama_print_timings:       total time =     564.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.93 ms /    27 runs   (    0.15 ms per token,  6870.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     972.92 ms /    27 runs   (   36.03 ms per token,    27.75 tokens per second)\n",
      "llama_print_timings:       total time =    1018.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    29 runs   (    0.15 ms per token,  6818.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    1046.97 ms /    29 runs   (   36.10 ms per token,    27.70 tokens per second)\n",
      "llama_print_timings:       total time =    1096.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     388.61 ms /   157 tokens (    2.48 ms per token,   404.00 tokens per second)\n",
      "llama_print_timings:        eval time =      73.20 ms /     2 runs   (   36.60 ms per token,    27.32 tokens per second)\n",
      "llama_print_timings:       total time =     466.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /    16 runs   (    0.15 ms per token,  6875.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     368.67 ms /   160 tokens (    2.30 ms per token,   433.99 tokens per second)\n",
      "llama_print_timings:        eval time =     540.51 ms /    15 runs   (   36.03 ms per token,    27.75 tokens per second)\n",
      "llama_print_timings:       total time =     936.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    15 runs   (    0.14 ms per token,  6912.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.89 ms /   122 tokens (    2.26 ms per token,   442.21 tokens per second)\n",
      "llama_print_timings:        eval time =     498.76 ms /    14 runs   (   35.63 ms per token,    28.07 tokens per second)\n",
      "llama_print_timings:       total time =     798.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     3 runs   (    0.15 ms per token,  6833.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     239.18 ms /    90 tokens (    2.66 ms per token,   376.28 tokens per second)\n",
      "llama_print_timings:        eval time =      69.62 ms /     2 runs   (   34.81 ms per token,    28.73 tokens per second)\n",
      "llama_print_timings:       total time =     313.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.88 ms /     6 runs   (    0.15 ms per token,  6802.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     285.28 ms /   122 tokens (    2.34 ms per token,   427.64 tokens per second)\n",
      "llama_print_timings:        eval time =     180.22 ms /     5 runs   (   36.04 ms per token,    27.74 tokens per second)\n",
      "llama_print_timings:       total time =     475.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /    29 runs   (    0.15 ms per token,  6875.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    1038.69 ms /    29 runs   (   35.82 ms per token,    27.92 tokens per second)\n",
      "llama_print_timings:       total time =    1087.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.88 ms /     6 runs   (    0.15 ms per token,  6825.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     214.07 ms /     6 runs   (   35.68 ms per token,    28.03 tokens per second)\n",
      "llama_print_timings:       total time =     224.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /    11 runs   (    0.15 ms per token,  6832.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.54 ms /   114 tokens (    2.36 ms per token,   424.51 tokens per second)\n",
      "llama_print_timings:        eval time =     346.94 ms /    10 runs   (   34.69 ms per token,    28.82 tokens per second)\n",
      "llama_print_timings:       total time =     633.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    15 runs   (    0.15 ms per token,  6849.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     296.94 ms /   122 tokens (    2.43 ms per token,   410.86 tokens per second)\n",
      "llama_print_timings:        eval time =     501.53 ms /    14 runs   (   35.82 ms per token,    27.91 tokens per second)\n",
      "llama_print_timings:       total time =     822.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.13 ms /    15 runs   (    0.14 ms per token,  7045.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.84 ms /    95 tokens (    2.72 ms per token,   367.02 tokens per second)\n",
      "llama_print_timings:        eval time =     490.82 ms /    14 runs   (   35.06 ms per token,    28.52 tokens per second)\n",
      "llama_print_timings:       total time =     774.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    29 runs   (    0.14 ms per token,  6962.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.03 ms /   122 tokens (    2.25 ms per token,   443.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1001.74 ms /    28 runs   (   35.78 ms per token,    27.95 tokens per second)\n",
      "llama_print_timings:       total time =    1325.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    15 runs   (    0.15 ms per token,  6830.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     532.50 ms /    15 runs   (   35.50 ms per token,    28.17 tokens per second)\n",
      "llama_print_timings:       total time =     557.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6688.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.22 ms /    61 tokens (    2.97 ms per token,   336.61 tokens per second)\n",
      "llama_print_timings:        eval time =      33.83 ms /     1 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =     218.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    15 runs   (    0.15 ms per token,  6874.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.76 ms /   119 tokens (    2.32 ms per token,   431.53 tokens per second)\n",
      "llama_print_timings:        eval time =     496.19 ms /    14 runs   (   35.44 ms per token,    28.21 tokens per second)\n",
      "llama_print_timings:       total time =     795.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    15 runs   (    0.14 ms per token,  6899.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     287.84 ms /   122 tokens (    2.36 ms per token,   423.85 tokens per second)\n",
      "llama_print_timings:        eval time =     498.38 ms /    14 runs   (   35.60 ms per token,    28.09 tokens per second)\n",
      "llama_print_timings:       total time =     810.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     701.19 ms\n",
      "llama_print_timings:      sample time =       3.44 ms /    24 runs   (    0.14 ms per token,  6972.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     855.61 ms /    24 runs   (   35.65 ms per token,    28.05 tokens per second)\n",
      "llama_print_timings:       total time =     895.53 ms\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for tweet in tweets:\n",
    "    prompt1 = \"Read the following tweet inside brackets:\\n\"\n",
    "    prompt2 = \"Classify the readed tweet as to whether it mentions sports betting, answer just yes or no.: \"\n",
    "    str_for_promp = ''\n",
    "    str_for_promp += prompt1 + '['+tweet+']' + '\\n' + prompt2\n",
    "    prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "    USER: {str_for_promp}\n",
    "\n",
    "    ASSISTANT:\n",
    "    '''\n",
    "    response=lcpp_llm(prompt=prompt_template, max_tokens=512, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)\n",
    "    to_filter = response[\"choices\"][0][\"text\"]\n",
    "    match = re.search(r'ASSISTANT:\\s*(.*)', generated_text, re.IGNORECASE)\n",
    "    answer = match.group(1).strip() if match else \"\"\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
